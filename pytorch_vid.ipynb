{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Bl7hr_Q_FvM5jWblZXUcIZWHZVgfnTFd",
      "authorship_tag": "ABX9TyPsPGmdpUAt5ZTDHdcQxS5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moostafaaa/human_activity_recognition/blob/main/pytorch_vid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsuqLeBchROa",
        "outputId": "a8e40d28-5038-47c1-cbf6-ed4d44adbba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MC3_18_Weights.KINETICS400_V1`. You can also use `weights=MC3_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mc3_18-a90a0ba3.pth\" to /root/.cache/torch/hub/checkpoints/mc3_18-a90a0ba3.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 74.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "mc3=torchvision.models.video.mc3_18(pretrained=True, progress=True)\n",
        "mean = [0.43216, 0.394666, 0.37645]\n",
        "std = [0.22803, 0.22145, 0.216989]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "Rku8kDMehybC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo)\n"
      ],
      "metadata": {
        "id": "l_x_LGGah4M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo)\n",
        "from typing import Dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-UuN5ldjDJ8",
        "outputId": "63cad9b4-5a94-47ec-91ef-14c02610264c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device on which to run the model\n",
        "# Set to cuda to load on GPU\n",
        "device = \"cuda\"\n",
        "\n",
        "# Pick a pretrained model and load the pretrained weights\n",
        "model_name = \"slowfast_r50\"\n",
        "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
        "\n",
        "# Set to eval mode and move to desired device\n",
        "model = model.to(device)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__ziRHUfktiE",
        "outputId": "9bb72dd6-1003-4cf8-fa1f-f9623fee81a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50.pyth\n",
            "100%|██████████| 264M/264M [00:01<00:00, 223MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwBRoSyrmnxM",
        "outputId": "f70f2a90-ab4f-4dac-cc9a-40e000040225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 08:48:37--  https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.249.141.13, 13.249.141.9, 13.249.141.40, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.249.141.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10326 (10K) [text/plain]\n",
            "Saving to: ‘kinetics_classnames.json’\n",
            "\n",
            "\rkinetics_classnames   0%[                    ]       0  --.-KB/s               \rkinetics_classnames 100%[===================>]  10.08K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-04-05 08:48:37 (8.53 MB/s) - ‘kinetics_classnames.json’ saved [10326/10326]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ],
      "metadata": {
        "id": "jUCt4Zi0liWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kinetics_classnames.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXMZjozmix3",
        "outputId": "d48cc75e-377a-44e4-daee-07fe2a97256d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('\"sharpening knives\"', 290), ('\"eating ice cream\"', 115), ('\"cutting nails\"', 81), ('\"changing wheel\"', 53), ('\"bench pressing\"', 19), ('deadlifting', 88), ('\"eating carrots\"', 111), ('marching', 192), ('\"throwing discus\"', 358), ('\"playing flute\"', 231), ('\"cooking on campfire\"', 72), ('\"breading or breadcrumbing\"', 33), ('\"playing badminton\"', 218), ('\"ripping paper\"', 276), ('\"playing saxophone\"', 244), ('\"milking cow\"', 197), ('\"juggling balls\"', 169), ('\"flying kite\"', 130), ('capoeira', 43), ('\"making jewelry\"', 187), ('drinking', 100), ('\"playing cymbals\"', 228), ('\"cleaning gutters\"', 61), ('\"hurling (sport)\"', 161), ('\"playing organ\"', 239), ('\"tossing coin\"', 361), ('wrestling', 395), ('\"driving car\"', 103), ('headbutting', 150), ('\"gymnastics tumbling\"', 147), ('\"making bed\"', 186), ('abseiling', 0), ('\"holding snake\"', 155), ('\"rock climbing\"', 278), ('\"cooking egg\"', 71), ('\"long jump\"', 182), ('\"bee keeping\"', 17), ('\"trimming or shaving beard\"', 365), ('\"cleaning shoes\"', 63), ('\"dancing gangnam style\"', 86), ('\"catching or throwing softball\"', 50), ('\"ice skating\"', 164), ('jogging', 168), ('\"eating spaghetti\"', 116), ('bobsledding', 28), ('\"assembling computer\"', 8), ('\"playing cricket\"', 227), ('\"playing monopoly\"', 238), ('\"golf putting\"', 143), ('\"making pizza\"', 188), ('\"javelin throw\"', 166), ('\"peeling potatoes\"', 211), ('clapping', 57), ('\"brushing hair\"', 36), ('\"flipping pancake\"', 129), ('\"drinking beer\"', 101), ('\"dribbling basketball\"', 99), ('\"playing bagpipes\"', 219), ('somersaulting', 325), ('\"canoeing or kayaking\"', 42), ('\"riding unicycle\"', 275), ('texting', 355), ('\"tasting beer\"', 352), ('\"hockey stop\"', 154), ('\"playing clarinet\"', 225), ('\"waxing legs\"', 389), ('\"curling hair\"', 80), ('\"running on treadmill\"', 281), ('\"tai chi\"', 346), ('\"driving tractor\"', 104), ('\"shaving legs\"', 293), ('\"sharpening pencil\"', 291), ('\"making sushi\"', 190), ('\"spray painting\"', 327), ('situp', 305), ('\"playing kickball\"', 237), ('\"sticking tongue out\"', 331), ('headbanging', 149), ('\"folding napkins\"', 132), ('\"playing piano\"', 241), ('skydiving', 312), ('\"dancing charleston\"', 85), ('\"ice fishing\"', 163), ('tickling', 359), ('bandaging', 13), ('\"high jump\"', 151), ('\"making a sandwich\"', 185), ('\"riding mountain bike\"', 271), ('\"cutting pineapple\"', 82), ('\"feeding goats\"', 125), ('\"dancing macarena\"', 87), ('\"playing basketball\"', 220), ('krumping', 179), ('\"high kick\"', 152), ('\"balloon blowing\"', 12), ('\"playing accordion\"', 217), ('\"playing chess\"', 224), ('\"hula hooping\"', 159), ('\"pushing wheelchair\"', 263), ('\"riding camel\"', 268), ('\"blowing out candles\"', 27), ('\"extinguishing fire\"', 121), ('\"using computer\"', 373), ('\"jumpstyle dancing\"', 173), ('yawning', 397), ('writing', 396), ('\"jumping into pool\"', 172), ('\"doing laundry\"', 96), ('\"egg hunting\"', 118), ('\"sanding floor\"', 284), ('\"moving furniture\"', 200), ('\"exercising arm\"', 119), ('\"sword fighting\"', 345), ('\"sign language interpreting\"', 303), ('\"counting money\"', 74), ('bartending', 15), ('\"cleaning windows\"', 65), ('\"blasting sand\"', 23), ('\"petting cat\"', 213), ('sniffing', 320), ('bowling', 31), ('\"playing poker\"', 242), ('\"taking a shower\"', 347), ('\"washing hands\"', 382), ('\"water sliding\"', 384), ('\"presenting weather forecast\"', 254), ('tobogganing', 360), ('celebrating', 51), ('\"getting a haircut\"', 138), ('snorkeling', 321), ('\"weaving basket\"', 390), ('\"playing squash or racquetball\"', 245), ('parasailing', 206), ('\"news anchoring\"', 202), ('\"belly dancing\"', 18), ('windsurfing', 393), ('\"braiding hair\"', 32), ('\"crossing river\"', 78), ('\"laying bricks\"', 181), ('\"roller skating\"', 280), ('hopscotch', 156), ('\"playing trumpet\"', 248), ('\"dying hair\"', 108), ('\"trimming trees\"', 366), ('\"pumping fist\"', 256), ('\"playing keyboard\"', 236), ('snowboarding', 322), ('\"garbage collecting\"', 136), ('\"playing controller\"', 226), ('dodgeball', 94), ('\"recording music\"', 266), ('\"country line dancing\"', 75), ('\"dancing ballet\"', 84), ('gargling', 137), ('ironing', 165), ('\"push up\"', 260), ('\"frying vegetables\"', 135), ('\"ski jumping\"', 307), ('\"mowing lawn\"', 201), ('\"getting a tattoo\"', 139), ('\"rock scissors paper\"', 279), ('cheerleading', 55), ('\"using remote controller (not gaming)\"', 374), ('\"shaking head\"', 289), ('sailing', 282), ('\"training dog\"', 363), ('hurdling', 160), ('\"fixing hair\"', 128), ('\"climbing ladder\"', 67), ('\"filling eyebrows\"', 126), ('\"springboard diving\"', 329), ('\"eating watermelon\"', 117), ('\"drumming fingers\"', 106), ('\"waxing back\"', 386), ('\"playing didgeridoo\"', 229), ('\"swimming backstroke\"', 339), ('\"biking through snow\"', 22), ('\"washing feet\"', 380), ('\"mopping floor\"', 198), ('\"throwing ball\"', 357), ('\"eating doughnuts\"', 113), ('\"drinking shots\"', 102), ('\"tying bow tie\"', 368), ('dining', 91), ('\"surfing water\"', 337), ('\"sweeping floor\"', 338), ('\"grooming dog\"', 145), ('\"catching fish\"', 47), ('\"pumping gas\"', 257), ('\"riding or walking with horse\"', 273), ('\"massaging person\\'s head\"', 196), ('archery', 5), ('\"ice climbing\"', 162), ('\"playing recorder\"', 243), ('\"decorating the christmas tree\"', 89), ('\"peeling apples\"', 210), ('snowmobiling', 324), ('\"playing ukulele\"', 249), ('\"eating burger\"', 109), ('\"building cabinet\"', 38), ('\"stomping grapes\"', 332), ('\"drop kicking\"', 105), ('\"passing American football (not in game)\"', 209), ('applauding', 3), ('hugging', 158), ('\"eating hotdog\"', 114), ('\"pole vault\"', 253), ('\"reading newspaper\"', 265), ('\"snatch weight lifting\"', 318), ('zumba', 399), ('\"playing ice hockey\"', 235), ('breakdancing', 34), ('\"feeding fish\"', 124), ('\"shredding paper\"', 300), ('\"catching or throwing frisbee\"', 49), ('\"exercising with an exercise ball\"', 120), ('\"pushing cart\"', 262), ('\"swimming butterfly stroke\"', 341), ('\"riding scooter\"', 274), ('spraying', 328), ('\"folding paper\"', 133), ('\"golf driving\"', 142), ('\"robot dancing\"', 277), ('\"bending back\"', 20), ('testifying', 354), ('\"waxing chest\"', 387), ('\"carving pumpkin\"', 46), ('\"hitting baseball\"', 153), ('\"riding elephant\"', 269), ('\"brushing teeth\"', 37), ('\"pull ups\"', 255), ('\"riding a bike\"', 267), ('skateboarding', 306), ('\"cleaning pool\"', 62), ('\"playing paintball\"', 240), ('\"massaging back\"', 193), ('\"shoveling snow\"', 299), ('\"surfing crowd\"', 336), ('unboxing', 371), ('faceplanting', 122), ('trapezing', 364), ('\"swinging legs\"', 343), ('hoverboarding', 157), ('\"playing violin\"', 250), ('\"wrapping present\"', 394), ('\"blowing nose\"', 26), ('\"kicking field goal\"', 174), ('\"picking fruit\"', 214), ('\"swinging on something\"', 344), ('\"giving or receiving award\"', 140), ('\"planting trees\"', 215), ('\"water skiing\"', 383), ('\"washing dishes\"', 379), ('\"punching bag\"', 258), ('\"massaging legs\"', 195), ('\"throwing axe\"', 356), ('\"salsa dancing\"', 283), ('bookbinding', 29), ('\"tying tie\"', 370), ('\"skiing crosscountry\"', 309), ('\"shining shoes\"', 295), ('\"making snowman\"', 189), ('\"front raises\"', 134), ('\"doing nails\"', 97), ('\"massaging feet\"', 194), ('\"playing drums\"', 230), ('smoking', 316), ('\"punching person (boxing)\"', 259), ('cartwheeling', 45), ('\"passing American football (in game)\"', 208), ('\"shaking hands\"', 288), ('plastering', 216), ('\"watering plants\"', 385), ('kissing', 176), ('slapping', 314), ('\"playing harmonica\"', 233), ('welding', 391), ('\"smoking hookah\"', 317), ('\"scrambling eggs\"', 285), ('\"cooking chicken\"', 70), ('\"pushing car\"', 261), ('\"opening bottle\"', 203), ('\"cooking sausages\"', 73), ('\"catching or throwing baseball\"', 48), ('\"swimming breast stroke\"', 340), ('digging', 90), ('\"playing xylophone\"', 252), ('\"doing aerobics\"', 95), ('\"playing trombone\"', 247), ('knitting', 178), ('\"waiting in line\"', 377), ('\"tossing salad\"', 362), ('squat', 330), ('vault', 376), ('\"using segway\"', 375), ('\"crawling baby\"', 77), ('\"reading book\"', 264), ('motorcycling', 199), ('barbequing', 14), ('\"cleaning floor\"', 60), ('\"playing cello\"', 223), ('drawing', 98), ('auctioning', 9), ('\"carrying baby\"', 44), ('\"diving cliff\"', 93), ('busking', 41), ('\"cutting watermelon\"', 83), ('\"scuba diving\"', 286), ('\"riding mechanical bull\"', 270), ('\"making tea\"', 191), ('\"playing tennis\"', 246), ('crying', 79), ('\"dunking basketball\"', 107), ('\"cracking neck\"', 76), ('\"arranging flowers\"', 7), ('\"building shed\"', 39), ('\"golf chipping\"', 141), ('\"tasting food\"', 353), ('\"shaving head\"', 292), ('\"answering questions\"', 2), ('\"climbing tree\"', 68), ('\"skipping rope\"', 311), ('kitesurfing', 177), ('\"juggling fire\"', 170), ('laughing', 180), ('paragliding', 205), ('\"contact juggling\"', 69), ('slacklining', 313), ('\"arm wrestling\"', 6), ('\"making a cake\"', 184), ('\"finger snapping\"', 127), ('\"grooming horse\"', 146), ('\"opening present\"', 204), ('\"tapping pen\"', 351), ('singing', 304), ('\"shot put\"', 298), ('\"cleaning toilet\"', 64), ('\"spinning poi\"', 326), ('\"setting table\"', 287), ('\"tying knot (not on a tie)\"', 369), ('\"blowing glass\"', 24), ('\"eating chips\"', 112), ('\"tap dancing\"', 349), ('\"climbing a rope\"', 66), ('\"brush painting\"', 35), ('\"chopping wood\"', 56), ('\"stretching leg\"', 334), ('\"petting animal (not cat)\"', 212), ('\"baking cookies\"', 11), ('\"stretching arm\"', 333), ('beatboxing', 16), ('jetskiing', 167), ('\"bending metal\"', 21), ('sneezing', 319), ('\"folding clothes\"', 131), ('\"sled dog racing\"', 315), ('\"tapping guitar\"', 350), ('\"bouncing on trampoline\"', 30), ('\"waxing eyebrows\"', 388), ('\"air drumming\"', 1), ('\"kicking soccer ball\"', 175), ('\"washing hair\"', 381), ('\"riding mule\"', 272), ('\"blowing leaves\"', 25), ('\"strumming guitar\"', 335), ('\"playing cards\"', 222), ('snowkiting', 323), ('\"playing bass guitar\"', 221), ('\"applying cream\"', 4), ('\"shooting basketball\"', 296), ('\"walking the dog\"', 378), ('\"triple jump\"', 367), ('\"shearing sheep\"', 294), ('\"clay pottery making\"', 58), ('\"bungee jumping\"', 40), ('\"unloading truck\"', 372), ('\"shuffling cards\"', 301), ('\"shooting goal (soccer)\"', 297), ('\"tango dancing\"', 348), ('\"side kick\"', 302), ('\"grinding meat\"', 144), ('yoga', 398), ('\"hammer throw\"', 148), ('\"changing oil\"', 52), ('\"checking tires\"', 54), ('parkour', 207), ('\"eating cake\"', 110), ('\"skiing slalom\"', 310), ('\"juggling soccer ball\"', 171), ('whistling', 392), ('\"feeding birds\"', 123), ('\"playing volleyball\"', 251), ('\"swing dancing\"', 342), ('\"skiing (not slalom or crosscountry)\"', 308), ('lunge', 183), ('\"disc golfing\"', 92), ('\"clean and jerk\"', 59), ('\"playing guitar\"', 232), ('\"baby waking up\"', 10), ('\"playing harp\"', 234)])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# SlowFast transform\n",
        "####################\n",
        "\n",
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "alpha = 4\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second"
      ],
      "metadata": {
        "id": "jFafSIh6msBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the example video\n",
        "video_path = \"/content/drive/MyDrive/Training/Tennis/v_tennis_01_02.mpg\"\n",
        "\n",
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = [i.to(device)[None, ...] for i in inputs]"
      ],
      "metadata": {
        "id": "TkouOUngnH_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the input clip through the model\n",
        "preds = model(inputs)"
      ],
      "metadata": {
        "id": "eDef401NneMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n",
        "print(\"Predicted labels: %s\" % \", \".join(pred_class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7oN-WJTnirS",
        "outputId": "9616e494-579d-4984-b9ce-78223b2e8636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: playing tennis, playing squash or racquetball, playing badminton, skipping rope, throwing ball\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYGzxZmpnm3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}